{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OPFSlZdUc08Z",
        "id6gSW4O2gVL"
      ],
      "authorship_tag": "ABX9TyPo1p3tQbCTnUkz7INNe7L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nacho2904/AnalisisNumerico/blob/main/ecus_lineales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sistemas de ecuaciones lineales"
      ],
      "metadata": {
        "id": "kjt1VmVZ0FnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Métodos directos"
      ],
      "metadata": {
        "id": "2CCz40mR6bvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buena parte de los métodos de resolución de ecuaciones lineales directos consisten en llevar una matriz cuadrada no singular $A \\in \\mathbb{R}^{n \\times n}$ a una matriz $U \\in \\mathbb{R}^{n \\times n}$, tal que:\n",
        "- $U = \\begin{bmatrix} a_{11} & a_{12} & \\ldots & a_{1n} \\\\ 0 & a_{22} & \\ldots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\ldots & a_{nn} \\end{bmatrix}$"
      ],
      "metadata": {
        "id": "TrHEjAuf07bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es decir, $U$ es **triangular superior**"
      ],
      "metadata": {
        "id": "wISGPW0L4Epx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teniendo la matriz de esa forma, es fácil ver que:\n",
        "- $Ux = \\begin{bmatrix} a_{11} & a_{12} & \\ldots & a_{1n} \\\\ 0 & a_{22} & \\ldots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\ldots & a_{nn} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{bmatrix} \\iff x_n = \\frac{b_n}{a_{nn}}, x_{n-1} = \\frac{b_{n-1} - a_{34}x_n}{a_{33}}, \\ldots, x_i = \\frac{b_i - \\sum_{j=i}^na_{ij}x_j}{a_{ii}}$"
      ],
      "metadata": {
        "id": "MH9OpELK4PTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este método para resolver sistemas de ecuaciones lineales en forma de matriz triangular superior se conoce como **back-substitution**. Para matrices triangulares inferiores se puede aplicar el mismo algoritmo trivialmente empezando desde la primer fila, en un proceso conocido como **forward-substitution**"
      ],
      "metadata": {
        "id": "k0CMwXkG4Wf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El método tradicional de dejar a la matriz original en su forma triangular superior es aplicando **Eliminación de Gauss** y su versión más avanzada **Eliminación de Gauss con pivoteo parcial**"
      ],
      "metadata": {
        "id": "7NhtmX8x817n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LU Decomposition"
      ],
      "metadata": {
        "id": "zZQ5FQz84Wc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La descomposición LU surge de la necesidad de resolver sucesivamente un sistema de ecuaciones lineales de manera que cada solución depende de un paso anterior. El objetivo es encontrar dos matrices $L, U \\in \\mathbb{R}^{n \\times n}$ tales que $L$ es triangular inferior, $U$ es triangular superior, y, siendo $A \\in \\mathbb{R}^{n \\times n}$ la matriz de coeficientes del sistema:\n",
        "- $A = LU \\Rightarrow Ax = b \\iff Ux = y, Ly = b$"
      ],
      "metadata": {
        "id": "WLnR31Sh8M4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota: Las ecuaciones de arriba implican multiplicar por $L^{-1}$ en ambos lados de la ecuación $LUx = b$, lo que levanta la pregunta de si $L$ es efectivamente inversible. La respuesta se encuentra en la propiedad:\n",
        "- Sean $A.B \\in \\mathbb{R}^{n \\times n}$, entonces se cumple que $rg(AB) \\leq min\\{rg(A), rg(B)\\}$. De aquí se deduce inmediatamente que $rg(A) = rg(LU) = n \\leq min(rg(L), rg(U)) \\iff rg(L) = rg(U) = n$ siempre y cuando $A$ sea una matriz no singular, es decir, $rg(A) = n$"
      ],
      "metadata": {
        "id": "zHgdigOWWTSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "De forma que si pudiéramos encontrar dichas matrices, el problema se transformaría en aplicar forward-substitution para encontrar $y$, y luego back-substitution para encontrar $x$."
      ],
      "metadata": {
        "id": "iGx9ckbg4Vzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ver como obtener ambas matrices, exponemos un ejemplo simple:\n",
        "- Sea $A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 2\\end{bmatrix}$."
      ],
      "metadata": {
        "id": "zKM2PHl4_o4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buscamos reducirla mediante eliminación gaussiana. Luego:\n",
        "- $A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 2\\end{bmatrix} \\underbrace{\\Rightarrow}_{R_2^{(2)} = R_2^{(1)} - 2R_1^{(1)}} \\begin{bmatrix} 1 & 2 \\\\ 0 & -2\\end{bmatrix}$"
      ],
      "metadata": {
        "id": "x552aZIuBWj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pero notamos que podemos representar ese cambio en la segunda fila mediante una multiplicación matricial:\n",
        "- $\\begin{bmatrix} 1 & 2 \\\\ 0 & -2\\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ -2 & 1\\end{bmatrix}\\begin{bmatrix} 1 & 2 \\\\ 2 & 2\\end{bmatrix} \\Rightarrow A = \\underbrace{\\begin{bmatrix} 1 & 0 \\\\ 2 & 1\\end{bmatrix}}_L\\underbrace{\\begin{bmatrix} 1 & 2 \\\\ 0 & -2\\end{bmatrix}}_U$ "
      ],
      "metadata": {
        "id": "cjAI0FywCKrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pensemos bien en qué representan respectivamente $L$ y $U$. $U$ es claramente el resultado de la aplicación de eliminación gaussiana a la matriz $A$. $L^{-1}$, como vemos en el lado izquierdo de la expresión descrita arriba, es una especie de \"historial\" de los cambios que fuimos haciendo a la matriz, mientras que $L$ nos da los pasos para reconstruir la matriz original a partir de las filas de $U$, en este caso sumando dos veces la primera fila y una vez la segunda."
      ],
      "metadata": {
        "id": "Qwuj-uQHDZI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Más generalmente:\n",
        "- $L = \\begin{bmatrix} 1 & 0 & 0 & \\ldots & 0\\\\ m_{21} & 1 & 0 & \\ldots & 0 \\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ m_{n-1,1} & \\ldots & m_{n-1,n-2} & 1 & 0 \\\\ m_{n1} & \\ldots & m_{n, n-2} & m_{n, n-1} & 1\\end{bmatrix}$"
      ],
      "metadata": {
        "id": "bvh7GHhIEKz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Donde $m_{ij}$ es el coeficiente correspondiente de la eliminación gaussiana realizada a A. Por ejemplo:\n",
        "- $A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 6 & 7 & 8 \\\\ 2 & 4 & 5\\end{bmatrix} \\underbrace{\\Rightarrow}_{R_2^{(2)} = R_2^{(1)} - 6R_1^{(1)} \\\\ R_3^{(2)} = R_3^{(1)} - 2R_1^{(1)}} \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -5 & -10 \\\\ 0 & 0 & -1\\end{bmatrix}$"
      ],
      "metadata": {
        "id": "1vjI3ecGHYnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notamos que allí, $m_{21} = 6, m_{31} = 2, m_{23} = 0, m_{32} = 0$. Luego:\n",
        "- $L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 6 & 1 & 0 \\\\ 2 & 0 & 1\\end{bmatrix} \\Rightarrow A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 6 & 7 & 8 \\\\ 2 & 4 & 5\\end{bmatrix}  =  \\underbrace{\\begin{bmatrix} 1 & 0 & 0 \\\\ 6 & 1 & 0 \\\\ 2 & 0 & 1\\end{bmatrix}}_L \\underbrace{\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -5 & -10 \\\\ 0 & 0 & -1\\end{bmatrix}}_U$"
      ],
      "metadata": {
        "id": "zIqFZ1ELJNDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenidas $L$ y $U$ procedemos por forward-substitution y back-substitution como ya hemos visto antes."
      ],
      "metadata": {
        "id": "nIaqHplYV57v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En caso de que hubiésemos tenido que permutar filas para obtener la matriz $U$, es decir, que hubiésemos usado pivoteo parcial, simplemente agregaríamos una matriz de permutación $P$ a la ecuación:\n",
        "- $PA = LU$"
      ],
      "metadata": {
        "id": "Ody4d9fWclDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Método de Cholesky"
      ],
      "metadata": {
        "id": "OPFSlZdUc08Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Digamos que además de tener una matriz de rango completo $A \\in \\mathbb{R}^{n \\times n}$, también asumimos que es definida positiva ($A ≻ 0$), y simétrica $(A^T = A)$. Por ejemplo:\n",
        "- $A = \\begin{bmatrix} 2 & 2 & 1 \\\\ 2 & 5 & 2 \\\\ 1 & 2 & 5 \\end{bmatrix}$"
      ],
      "metadata": {
        "id": "YwAvI2k6g1gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculando su descomposición $LU$ obtenemos que:\n",
        "- $A = \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ \\frac{1}{2} & \\frac{1}{3} & 1\\end{bmatrix}\\begin{bmatrix} 2 & 2 & 1 \\\\ 0 & 3 & 1 \\\\ 0 & 0 & \\frac{25}{6}\\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ \\frac{1}{2} & \\frac{1}{3} & 1\\end{bmatrix}\\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & \\frac{25}{6}\\end{bmatrix}\\begin{bmatrix} 1 & 1 & \\frac{1}{2} \\\\ 0 & 1 & \\frac{1}{3} \\\\ 0 & 0 & 1\\end{bmatrix} = LDL^T$"
      ],
      "metadata": {
        "id": "c9kmptoMiNi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta descomposición, intuitivamente conocida como **descomposición LDL**, es propia de todas las matrices simétricas definidas positivas. Yendo un paso más allá, podemos definir $\\sqrt{D}: \\sqrt{D}\\sqrt{D} = D$ como la matriz de las raíces de cada elemento de la diagonal de $D$. Entonces: \n",
        "- $A = LDL^T = L\\sqrt{D}\\sqrt{D}L^T = \\underbrace{L\\sqrt{D}}_S(L\\sqrt{D})^T = SS^T \\Rightarrow A = SS^T$"
      ],
      "metadata": {
        "id": "oxpzhZiBi090"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta descomposición se conoce como **descomposición de Cholesky**, y es la base del **algoritmo de Cholesky** para resolver sistemas de ecuaciones lineales con matriz de coeficientes simétrica definida positiva. Una vez obtenida la descomposición de Cholesky, nos queda el sistema resultante:\n",
        "- $S^Tx = y, Sy = b$"
      ],
      "metadata": {
        "id": "31fi6tVskuEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que podemos resolver por forward-substitution seguido de back-substitution, y tiene la ventaja de que solo necesitamos almacenar una matriz en vez de dos como teníamos con la descomposición $LU$."
      ],
      "metadata": {
        "id": "7FB0C8ZYlUce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Métodos iterativos"
      ],
      "metadata": {
        "id": "id6gSW4O2gVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los métodos que resumimos en el apartado anterior se conocen como *métodos directos* pues tienen una cantidad finita de pasos y dan resultados que teóricamente deberían ser exactos. Estos métodos de suelen utilizar con matrices densas, es decir, con muchos coeficientes distintos de cero."
      ],
      "metadata": {
        "id": "PGuwk3aE2nJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin embargo, cuando tratamos con matrices **dispersas**, al utilizar métodos directos estaremos introduciendo, en las entradas que antes eran nulas, errores que antes no existían. Los métodos **iterativos** tienen en cuenta este tipo de matrices. En estos métodos obtenemos la solución a partir de una solución inicial que vamos refinando hasta obtener la solución correcta."
      ],
      "metadata": {
        "id": "FzLsOqR-3HRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sabemos que nuestro sistema se puede expresar de la forma\n",
        "- $Ax = B \\Rightarrow  B - Ax = 0$"
      ],
      "metadata": {
        "id": "77XkZiuu3rKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego podemos sumar $Mx$ en ambos miembros de la igualdad. Obtenemos:\n",
        "- $B - Ax + Mx = Mx \\iff Mx = B + (M-A)x \\iff x = M^{-1}B + M^{-1}(M-A)x$"
      ],
      "metadata": {
        "id": "abwqfL-j3rbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Renombrando algunas variables y reacomodando la ecuación, obtenemos:\n",
        "- $x^{(i+1)} = (I - M^{-1}A)x^{(n)} + M^{-1}B$"
      ],
      "metadata": {
        "id": "UOZt80xo4EQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que puede escribirse de una forma más general como\n",
        "- $x^{(i+1)} = Tx^{(n)} + C$"
      ],
      "metadata": {
        "id": "Q7TRnY4R3rei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esta última expresión podemos definir dos tipos de métodos iterativos: los métodos **estacionarios**, en los que $T$ y $C$ no sufren modificaciones durante las iteraciones, y los **no estacionarios**, aquellos en los que los valores de $T$ y $C$ dependen de la iteración. "
      ],
      "metadata": {
        "id": "kO_5OrI63rhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Métodos iterativos estacionarios"
      ],
      "metadata": {
        "id": "052jZbdf68bA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supongamos que conocemos nuestra solución exacta $\\bar{x}$. Entonces podemos decir que:\n",
        "- $\\bar{x} = x^{(n+1)} + e^{(n+1)} = T(x^{(n)} + e^{(n)}) + C = \\underbrace{Tx^{(n)} + C}_{ x^{(n+1)}} + Te^{(n)} =  x^{(n+1)} +  Te^{(n)}$"
      ],
      "metadata": {
        "id": "83gmBsbl7E6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego:\n",
        "- $e^{(n+1)} = Te^{(n)} ⇒ e^{(n+1)} = T^{n+1}e^{(0)}$"
      ],
      "metadata": {
        "id": "FgekyTda8ZnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo que nos indica que para un método iterativo estacionario sea convergente se debe cumplir que $||T|| < 1$."
      ],
      "metadata": {
        "id": "lxq2zbGT8rPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Método de Jacobi"
      ],
      "metadata": {
        "id": "zyTo_QXC9HjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es evidente que podemos descomponer cualquier matriz cuadrada $A$ de la forma:\n",
        "- $A = L + D + U$"
      ],
      "metadata": {
        "id": "ewNoAAJA9KCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En donde $L$ es una matriz triangular inferior con diagonal nula, $D$ es una matriz diagonal y $U$ es una matriz triangular superior con diagonal nula. Si definimos $M = D$ para nuestro método iterativo, obtenemos:\n",
        "- $x^{(i+1)} = (I - D^{-1}A)x^{(n)} + D^{-1}B = (I - D^{-1}(L + D + U))x^{(n)} + D^{-1}B = (-D^{-1}(L+U))x^{(n)} + D^{-1}B$"
      ],
      "metadata": {
        "id": "DM6RIy529mQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, obtenemos la expresión:\n",
        "- $x^{(i+1)} = D^{-1}(B-(L+U)x^{(n)})$"
      ],
      "metadata": {
        "id": "PlXAoiBV-wL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El método "
      ],
      "metadata": {
        "id": "m9N3V4JBC-cY"
      }
    }
  ]
}